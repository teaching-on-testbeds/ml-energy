{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network Classifier\n",
    "\n",
    "Following the example in the demos you have seen, clear the keras session. Then, create a neural network `model` with:\n",
    "\n",
    "-   `nh=256` hidden units in a single dense hidden layer\n",
    "-   `sigmoid` activation at hidden units\n",
    "-   select the input and output shapes, and output activation, according to the problem requirements. Use the variables you defined earlier (`n_tr`, `n_ts`, `n_feat`, `n_class`) as applicable, rather than hard-coding numbers."
   ],
   "id": "1c959b9e-c9b9-4332-a118-55caa4da81ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline"
   ],
   "id": "511e34b8-e54c-4d96-a206-fc1ddf41fd9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Here, we’ll load the processed data defined in the previous notebook"
   ],
   "id": "d3fd8e98-cfb1-422a-98fe-e930b75a20c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_scale = np.load('instrument_dataset/uiowa_std_scale_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_permuted_train_labels.npy')\n",
    "Xts_scale = np.load('instrument_dataset/uiowa_std_scale_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ],
   "id": "afc3c242-a1c0-4687-8ff9-199406eb30ae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classification model"
   ],
   "id": "2dadf2d7-4182-4ac9-860e-2a85ce5a969a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow.keras.backend as K"
   ],
   "id": "46b581d4-4f25-471c-ae6b-9e792ef98ee6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - construct the model\n",
    "nh = 256\n",
    "model = Sequential()\n",
    "model.add(Input(Xtr_scale.shape[1],))\n",
    "model.add(Dense(nh, activation = 'sigmoid'))\n",
    "model.add(Dense(len(np.unique(ytr)), activation = 'softmax'))"
   ],
   "id": "032b583b-8d8d-42ee-b8a3-cc36c5d987b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model summary."
   ],
   "id": "9ddda2ac-e74c-407e-b432-f47b22d53755"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model summary\n",
    "model.summary()"
   ],
   "id": "aa7af715-d454-47b5-9526-962b2dc4eb8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also visualize the model with\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ],
   "id": "70ee9e75-dc3f-4eda-a1f2-fd1d830dcbb1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer and compile the model. Select the appropriate loss function for this multi-class classification problem, and use an accuracy metric. For the optimizer, use the Adam optimizer with a learning rate of 0.001"
   ],
   "id": "4b7062b6-e473-4000-8d2a-c233afee1294"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - create optimizer and compile the model\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])"
   ],
   "id": "965be741-37d4-4185-969e-17848c2e95e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for 10 epochs using the scaled data for both training and validation, and save the training history in \\`hist.\n",
    "\n",
    "Use the `validation_data` option to pass the *test* data. (This is OK because we are not going to use this data as part of the training process, such as for early stopping - we’re just going to compute the accuracy on the data so that we can see how training and test loss changes as the model is trained.)\n",
    "\n",
    "Use a batch size of 128. Your final accuracy should be greater than 99%."
   ],
   "id": "e5bc0909-3353-4df3-949a-987c7dc73e4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fit model and save training history\n",
    "n_epochs = 10\n",
    "\n",
    "hist = model.fit(Xtr_scale,ytr, batch_size = 128, epochs = 10, validation_data=(Xts_scale, yts))"
   ],
   "id": "70319ccd-1b38-460e-9b55-860a3b5f9e39"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy saved in `hist.history` dictionary, on the same plot. This gives one accuracy value per epoch. You should see that the validation accuracy saturates around 99%. After that it may “bounce around” a little due to the noise in the stochastic mini-batch gradient descent.\n",
    "\n",
    "Make sure to label each axis, and each series (training vs. validation/test)."
   ],
   "id": "b5263e8d-d49e-4d8f-bceb-f49b875552d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - plot the training and validation accuracy in one plot\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "train_acc = hist.history['accuracy'];\n",
    "val_acc = hist.history['val_accuracy'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "plt1 = sns.lineplot(x=np.arange(1,nepochs+1), y=train_acc, label='Training accuracy');\n",
    "plt2 = sns.lineplot(x=np.arange(1,nepochs+1), y=val_acc, label='Validation accuracy');\n",
    "xlab = plt.xlabel('Epoch');\n",
    "ylab = plt.ylabel('Accuracy')"
   ],
   "id": "11736ac5-ae37-412d-9bf7-525c5b12439b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss values saved in the `hist.history` dictionary, on the same plot. You should see that the training loss is steadily decreasing. Use the [`semilogy` plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogy.html) so that the y-axis is log scale.\n",
    "\n",
    "Make sure to label each axis, and each series (training vs. validation/test)."
   ],
   "id": "cb0803ed-ca2f-4dfa-a47c-e599324809b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - plot the training and validation loss in one plot\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "train_loss = hist.history['loss'];\n",
    "val_loss = hist.history['val_loss'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "plt1 = plt.semilogy(np.arange(1,nepochs+1), train_loss, label='Training loss');\n",
    "plt2 = plt.semilogy(np.arange(1,nepochs+1), val_loss, label='Validation loss');\n",
    "xlab = plt.xlabel('Epoch')\n",
    "ylab = plt.ylabel('Loss')\n",
    "lgnd = plt.legend()"
   ],
   "id": "697ffee3-2a4f-471a-8339-120f1c2caff2"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
