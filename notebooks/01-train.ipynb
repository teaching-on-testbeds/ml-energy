{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network Classifier\n",
    "\n",
    "Create a neural network `model` with:\n",
    "\n",
    "-   `nh=256` hidden units in a single dense hidden layer\n",
    "-   `sigmoid` activation at hidden units\n",
    "-   select the input and output shapes, and output activation, according\n",
    "    to the problem requirements."
   ],
   "id": "b6b05c96-b055-45c4-9164-3cd2ccec3396"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "id": "44bc6e33-4353-42be-9d2b-83379d3c61ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Here, we’ll load the processed data defined in the previous notebook"
   ],
   "id": "d2b48e13-7a6f-4b66-8083-d520cb6a1ca0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_scale = np.load('instrument_dataset/uiowa_std_scale_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_permuted_train_labels.npy')\n",
    "Xts_scale = np.load('instrument_dataset/uiowa_std_scale_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ],
   "id": "3c67346c-916f-4793-8adf-4cf31e643928"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classification model"
   ],
   "id": "e097892a-d172-4b4f-8b99-56a5c69b7245"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow.keras.backend as K"
   ],
   "id": "7279bebd-0ec7-4e86-9c91-9e9a4171b1c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - construct the model\n",
    "nh = 256\n",
    "model = Sequential()\n",
    "model.add(Input(Xtr_scale.shape[1],))\n",
    "model.add(Dense(nh, activation = 'sigmoid'))\n",
    "model.add(Dense(len(np.unique(ytr)), activation = 'softmax'))"
   ],
   "id": "3f4970c0-12d5-4054-842f-48d82eff0027"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model summary."
   ],
   "id": "c8f7092d-26c4-49bd-965c-106373904e05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model summary\n",
    "model.summary()"
   ],
   "id": "68993d27-0b9b-4f60-9178-fce08114a382"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also visualize the model with\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ],
   "id": "12682cb9-d1ca-42fa-9d5b-f313cd79b4c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer and compile the model. Select the appropriate loss\n",
    "function for this multi-class classification problem, and use an\n",
    "accuracy metric. For the optimizer, use the Adam optimizer with a\n",
    "learning rate of 0.001"
   ],
   "id": "12d9d1a8-48b9-4fa6-b559-f655b4ecda0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - create optimizer and compile the model\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])"
   ],
   "id": "35e71644-0c13-444a-803e-10bd6b9fc0f0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for 10 epochs using the scaled data for both training and\n",
    "validation, and save the training history in \\`hist.\n",
    "\n",
    "Use the `validation_data` option to pass the *test* data. (This is OK\n",
    "because we are not going to use this data as part of the training\n",
    "process, such as for early stopping - we’re just going to compute the\n",
    "accuracy on the data so that we can see how training and test loss\n",
    "changes as the model is trained.)\n",
    "\n",
    "Use a batch size of 128. Your final accuracy should be greater than 99%."
   ],
   "id": "5f80d868-2956-4a7f-826c-6fd8f74320ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fit model and save training history\n",
    "n_epochs = 10\n",
    "\n",
    "hist = model.fit(Xtr_scale,ytr, batch_size = 128, epochs = 10, validation_data=(Xts_scale, yts))"
   ],
   "id": "846a1119-e2d3-48bd-a027-58db151b6e7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy saved in `hist.history`\n",
    "dictionary, on the same plot. This gives one accuracy value per epoch.\n",
    "You should see that the validation accuracy saturates around 99%. After\n",
    "that it may “bounce around” a little due to the noise in the stochastic\n",
    "mini-batch gradient descent.\n",
    "\n",
    "Make sure to label each axis, and each series (training\n",
    "vs. validation/test)."
   ],
   "id": "a2fac846-8a60-4d5c-af2d-9c05a68d110f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - plot the training and validation accuracy in one plot\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "train_acc = hist.history['accuracy'];\n",
    "val_acc = hist.history['val_accuracy'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "plt1 = sns.lineplot(x=np.arange(1,nepochs+1), y=train_acc, label='Training accuracy');\n",
    "plt2 = sns.lineplot(x=np.arange(1,nepochs+1), y=val_acc, label='Validation accuracy');\n",
    "xlab = plt.xlabel('Epoch');\n",
    "ylab = plt.ylabel('Accuracy')"
   ],
   "id": "800c3ffe-b05e-406f-81c6-5a2f5995fb86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss values saved in the `hist.history`\n",
    "dictionary, on the same plot. You should see that the training loss is\n",
    "steadily decreasing. Use the [`semilogy`\n",
    "plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogy.html)\n",
    "so that the y-axis is log scale.\n",
    "\n",
    "Make sure to label each axis, and each series (training\n",
    "vs. validation/test)."
   ],
   "id": "45abfdb6-ab06-4897-abc6-0840d2809598"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - plot the training and validation loss in one plot\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "train_loss = hist.history['loss'];\n",
    "val_loss = hist.history['val_loss'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "plt1 = plt.semilogy(np.arange(1,nepochs+1), train_loss, label='Training loss');\n",
    "plt2 = plt.semilogy(np.arange(1,nepochs+1), val_loss, label='Validation loss');\n",
    "xlab = plt.xlabel('Epoch')\n",
    "ylab = plt.ylabel('Loss')\n",
    "lgnd = plt.legend()"
   ],
   "id": "cd063812-5d98-4e41-99fc-1816a47e8b21"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
