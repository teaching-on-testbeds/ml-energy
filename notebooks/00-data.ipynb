{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c4b412-75bb-41fa-9c87-4f745f8caf42",
   "metadata": {},
   "source": [
    "# Load data for music classification\n",
    "\n",
    "**Note**: This experiment is designed to run on a Chameleon **GPU**\n",
    "runtime. You should use a GPU runtime launched on the Chameleon server\n",
    "to work on this assignment. Refer to the `reserve.ipynb` notebook for\n",
    "instructions on how to reserve resources and launch an instance on the\n",
    "Chameleon server.\n",
    "\n",
    "In this assignment, we will look at an audio classification problem.\n",
    "Given a sample of music, we want to determine which instrument\n",
    "(e.g.¬†trumpet, violin, piano) is playing.\n",
    "\n",
    "*This assignment is closely based on one by Sundeep Rangan, from his\n",
    "[IntroML GitHub repo](https://github.com/sdrangan/introml/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1691a64-f81f-457b-b417-51abdabddec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d780886-2dce-4379-8941-4572cd3bc0ba",
   "metadata": {},
   "source": [
    "## Audio Feature Extraction with Librosa\n",
    "\n",
    "The key to audio classification is to extract the correct features. The\n",
    "`librosa` package in python has a rich set of methods for extracting the\n",
    "features of audio samples commonly used in machine learning tasks, such\n",
    "as speech recognition and sound classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea00c73-4a90-4513-9118-1e20971e2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d50bc8-e956-4f3e-9a88-dbc397df7a61",
   "metadata": {},
   "source": [
    "In this lab, we will use a set of music samples from the website:\n",
    "\n",
    "http://theremin.music.uiowa.edu\n",
    "\n",
    "This website has a great set of samples for audio processing.\n",
    "\n",
    "We will use the `wget` command to retrieve one file to our Google Colab\n",
    "storage area. (We can run `wget` and many other basic Linux commands in\n",
    "Colab by prefixing them with a `!` or `%`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55972de5-b872-420d-9428-8291331ecc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/SopSax.Vib.pp.C6Eb6.aiff\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc89da-9910-4c94-957a-4b9a77eb495a",
   "metadata": {},
   "source": [
    "Now, if you click on the small folder icon on the far left of the Colab\n",
    "interface, you can see the files in your Colab storage. You should see\n",
    "the ‚ÄúSopSax.Vib.pp.C6Eb6.aiff‚Äù file appear there.\n",
    "\n",
    "In order to listen to this file, we‚Äôll first convert it into the `wav`\n",
    "format. Again, we‚Äôll use a magic command to run a basic command-line\n",
    "utility: `ffmpeg`, a powerful tool for working with audio and video\n",
    "files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ef564-49af-41e1-aee3-2f4a3f8b1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiff_file = 'SopSax.Vib.pp.C6Eb6.aiff'\n",
    "wav_file = 'SopSax.Vib.pp.C6Eb6.wav'\n",
    "\n",
    "!ffmpeg -y -i $aiff_file $wav_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b56700-16cd-42ca-b48c-5f62ecac2773",
   "metadata": {},
   "source": [
    "Now, we can play the file directly from the Jupyter Notebook interface.\n",
    "If you press the ‚ñ∂Ô∏è button, you will hear a soprano saxaphone (with\n",
    "vibrato) playing four notes (C, C#, D, Eb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef13c5-73f3-47a0-8bc2-8fa9cd940f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(wav_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f5413-820e-4818-80ed-19a52d43f249",
   "metadata": {},
   "source": [
    "Next, use `librosa` command `librosa.load` to read the audio file with\n",
    "filename `audio_file` and get the samples `y` and sample rate `sr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe6204-dab2-4b5a-87a4-5fd11c61a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(aiff_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814bb918-b0db-4ad5-9dce-e7eae05ad37e",
   "metadata": {},
   "source": [
    "Feature engineering from audio files is an entire subject in its own\n",
    "right. A commonly used set of features are called the Mel Frequency\n",
    "Cepstral Coefficients (MFCCs). These are derived from the so-called mel\n",
    "spectrogram, which is something like a regular spectrogram, but the\n",
    "power and frequency are represented in log scale, which more naturally\n",
    "aligns with human perceptual processing.\n",
    "\n",
    "You can run the code below to display the mel spectrogram from the audio\n",
    "sample.\n",
    "\n",
    "You can easily see the four notes played in the audio track. You also\n",
    "see the ‚Äòharmonics‚Äô of each notes, which are other tones at integer\n",
    "multiples of the fundamental frequency of each note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b8e1e-00b3-44a3-bffd-3708500d9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S),\n",
    "                         y_axis='mel', fmax=8000, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cfdfd-fd36-445f-9033-c230c9cae3ed",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "Using the MFCC features described above, [Prof.¬†Juan\n",
    "Bello](http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello) at NYU\n",
    "Steinhardt and his former PhD student Eric Humphrey have created a\n",
    "complete data set that can used for instrument classification.\n",
    "Essentially, they collected a number of data files from the website\n",
    "above. For each audio file, the segmented the track into notes and then\n",
    "extracted 120 MFCCs for each note. The goal is to recognize the\n",
    "instrument from the 120 MFCCs. The process of feature extraction is\n",
    "quite involved. So, we will just use their processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15aedc9-583c-4116-a9e7-21dc36012f84",
   "metadata": {},
   "source": [
    "To retrieve their data, visit\n",
    "\n",
    "<https://github.com/marl/dl4mir-tutorial/tree/master>\n",
    "\n",
    "and note the password listed on that page. Click on the link for\n",
    "‚ÄúInstrument Dataset‚Äù, enter the password, click on `instrument_dataset`\n",
    "to open the folder, and download it. (You can ‚Äúdirect download‚Äù straight\n",
    "from this site, you don‚Äôt need a Dropbox account.) Depending on your\n",
    "laptop OS and on how you download the data, you may need to ‚Äúunzip‚Äù or\n",
    "otherwise extract the four `.npy` files from an archive.\n",
    "\n",
    "Now create a new folder (named `instrument_dataset`) on the Chameleon\n",
    "server for store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a35bea-ad04-498d-ba68-1a6e95855049",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir instrument_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562587c5-f8a7-4cdb-92a6-761e3d33622e",
   "metadata": {},
   "source": [
    "Then, upload the files to Chamelen server inside the\n",
    "`instrument_dataset` folder: click on the folder icon on the left to see\n",
    "your storage, if it isn‚Äôt already open, and then click on ‚ÄúUpload‚Äù.\n",
    "\n",
    "üõë Wait until *all* uploads have completed and the ‚ÄúLast Modified‚Äù\n",
    "attribute of the file *stops updating* indicating uploads in progress\n",
    "are *complete*. (The training data especially will take some time to\n",
    "upload.) üõë"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1b357-57c0-4657-87d5-8b7c9320e79d",
   "metadata": {},
   "source": [
    "Then, load the files with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e60dbe-931c-484d-adf1-fa7e4b6f415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.load('instrument_dataset/uiowa_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_train_labels.npy')\n",
    "Xts = np.load('instrument_dataset/uiowa_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267aac23-26ed-4f0c-bd11-77468f9160d1",
   "metadata": {},
   "source": [
    "Examine the data you have just loaded in:\n",
    "\n",
    "-   How many training samples are there?\n",
    "-   How many test samples are there?\n",
    "-   What is the number of features for each sample?\n",
    "-   How many classes (i.e.¬†instruments) are there?\n",
    "\n",
    "Write some code to find these values and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6078a-6e5b-4e9c-b94a-a91370642ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -  get basic details of the data\n",
    "# compute these values from the data, don't hard-code them\n",
    "n_tr    = Xtr.shape[0]\n",
    "n_ts    = Xts.shape[0]\n",
    "n_feat  = Xtr.shape[1]\n",
    "n_class = len(np.unique(ytr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02785cc-592f-4523-8baa-7b11aacf9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now print those details\n",
    "print(\"Num training= %d\" % n_tr)\n",
    "print(\"Num test=     %d\" % n_ts)\n",
    "print(\"Num features= %d\" % n_feat)\n",
    "print(\"Num classes=  %d\" % n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde9273-0388-4098-99d0-f615abf76382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training set\n",
    "# (when loaded in, samples are ordered by class)\n",
    "p = np.random.permutation(Xtr.shape[0])\n",
    "Xtr = Xtr[p,:]\n",
    "ytr = ytr[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a7d0e-a4ab-410d-be3d-92b7620aa08d",
   "metadata": {},
   "source": [
    "Then, standardize the training and test data, `Xtr` and `Xts`, by\n",
    "removing the mean of each feature and scaling to unit variance.\n",
    "\n",
    "You can do this manually, or using `sklearn`‚Äôs\n",
    "[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "(For an example showing how to use a `StandardScaler`, you can refer to\n",
    "the notebook on regularization.)\n",
    "\n",
    "Although you will scale both the training and test data, you should make\n",
    "sure that both are scaled according to the mean and variance statistics\n",
    "from the *training data only*.\n",
    "\n",
    "<small>Standardizing the input data can make the gradient descent work\n",
    "better, by making the loss function ‚Äúeasier‚Äù to descend.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf94e2-9886-4e99-a5e4-9db121595fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffe4ef-1616-4205-8fd9-fcf8618c648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65523996-7d01-4b2b-a743-00ab8dd67d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Standardize the training and test data\n",
    "Xtr_scale = scaler.fit_transform(Xtr)\n",
    "Xts_scale = scaler.transform(Xts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8b09e-703c-42ac-961e-790be1655fe8",
   "metadata": {},
   "source": [
    "Saving the standardized training and test data features for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3e699-b8b8-4f69-89bf-fd36ae3e2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('instrument_dataset/uiowa_std_scale_train_data.npy',Xtr_scale)\n",
    "np.save('instrument_dataset/uiowa_std_scale_test_data.npy',Xts_scale)\n",
    "np.save('instrument_dataset/uiowa_permuted_train_labels.npy',ytr)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
