{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2c4d91-2396-47ff-986a-b7e990d4a4a5",
   "metadata": {},
   "source": [
    "## Energy and time to train a neural network\n",
    "\n",
    "In the previous notebook, we trained a model to classify music. Now,\n",
    "we’ll explore what happens when we vary the training hyperparameters,\n",
    "but train each model to the same validation **accuracy target**. We will\n",
    "consider:\n",
    "\n",
    "-   how much *time* it takes to achieve that accuracy target (“time to\n",
    "    accuracy”)\n",
    "-   how much *energy* it takes to achieve that accuracy target (“energy\n",
    "    to accuracy”)\n",
    "-   and the *test accuracy* for the model, given that it is trained to\n",
    "    the specified validation accuracy target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193bdb4-d9c0-479e-995f-1985274b026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8564013-d187-40c0-a02f-e74bce6fe6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851e286-2840-4766-be9e-a1ed7db1dd2e",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Here, we’ll load the processed data defined in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0eac2-7845-4b0f-9f5e-2f6963208734",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_scale = np.load('instrument_dataset/uiowa_std_scale_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_permuted_train_labels.npy')\n",
    "Xts_scale = np.load('instrument_dataset/uiowa_std_scale_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2fd117-d059-4176-9533-6f460b5acece",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 256 #Number of units for the hidden layer of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87100025-ff25-4103-819d-bccf095036e6",
   "metadata": {},
   "source": [
    "### Energy consumption\n",
    "\n",
    "To do this, first we will need some way to measure the energy used to\n",
    "train the model. We will use [Zeus](https://ml.energy/zeus/overview/), a\n",
    "Python package developed by researchers at the University of Michigan,\n",
    "to measure the GPU energy consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17840dae-2165-4db7-b5fd-0ba17d0dbe6a",
   "metadata": {},
   "source": [
    "Import the zeus-ml package, and tell it to monitor your GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6b6e2-6aa4-4327-bc23-b975a2e2b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeus.monitor import ZeusMonitor\n",
    "\n",
    "monitor = ZeusMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205ce4c-d60b-4dfe-a187-3d8c302c0eca",
   "metadata": {},
   "source": [
    "When you want to measure GPU energy usage, you will:\n",
    "\n",
    "-   start a “monitoring window”\n",
    "-   do your GPU-intensive computation (e.g. call `model.fit`)\n",
    "-   stop the “monitoring window”\n",
    "\n",
    "and then you can get the time and total energy used by the GPU in the\n",
    "monitoring window.\n",
    "\n",
    "Try it now - this will just continue fitting whatever `model` is\n",
    "currently in scope from previous cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08033dc6-c0d5-451f-80e1-3e31e4fcd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((Xtr_scale.shape[1],)))\n",
    "model.add(Dense(nh, activation = 'sigmoid'))\n",
    "model.add(Dense(len(np.unique(ytr)), activation = 'softmax'))\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "try:\n",
    "    monitor.begin_window(\"test\")\n",
    "# if the last measurement window is still running\n",
    "except ValueError:\n",
    "    _ = monitor.end_window(\"test\")\n",
    "    monitor.begin_window(\"test\")\n",
    "\n",
    "model.fit(Xtr_scale, ytr, epochs=5)\n",
    "measurement = monitor.end_window(\"test\")\n",
    "print(\"Measured time (s)  :\" , measurement.time)\n",
    "print(\"Measured energy (J):\" , measurement.total_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f3013-03e5-4240-90db-3c373e5163b9",
   "metadata": {},
   "source": [
    "### `TrainToAccuracy` callback\n",
    "\n",
    "Next, we need a way to train a model until we achieve our desired\n",
    "validation accuracy. We will [write a callback\n",
    "function](https://www.tensorflow.org/guide/keras/writing_your_own_callbacks)\n",
    "following these specifications:\n",
    "\n",
    "-   It will be called `TrainToAccuracy` and will accept two arguments: a\n",
    "    `threshold` and a `patience` value.\n",
    "-   If the model’s validation accuracy is higher than the `threshold`\n",
    "    for `patience` epochs in a row, stop training.\n",
    "-   In the `on_epoch_end` function, which will be called at the end of\n",
    "    every epoch during training, you should get the current validation\n",
    "    accuracy using `currect_acc = logs.get(\"val_accuracy\")`. Then, set\n",
    "    `self.model.stop_training = True` if the condition above is met.\n",
    "-   The default values of `threshold` and `patience` are given below,\n",
    "    but other values may be passed as arguments at runtime.\n",
    "\n",
    "Then, when you call `model.fit()`, you will add the `TrainToAccuracy`\n",
    "callback as in\n",
    "\n",
    "    callbacks=[TrainToAccuracy(threshold=0.98, patience=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f2b5d-2b8b-48f8-9d5d-2ec53c68c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - write a callback function\n",
    "class TrainToAccuracy(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, threshold=0.9, patience=3):\n",
    "        self.threshold = threshold  # the desired accuracy threshold\n",
    "        self.patience = patience # how many epochs to wait once hitting the threshold\n",
    "        self.last_change_epoch = -1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_acc = logs.get(\"val_accuracy\")\n",
    "        if(current_acc>self.threshold):\n",
    "          if(epoch - self.last_change_epoch>=self.patience):\n",
    "            self.model.stop_training = True\n",
    "        else:\n",
    "          self.last_change_epoch = epoch\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103f2a7-ad7d-49c0-b639-c55645a534a1",
   "metadata": {},
   "source": [
    "Try it! run the following cell to test your `TrainToAccuracy` callback.\n",
    "(This will just continue fitting whatever `model` is currently in\n",
    "scope.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ccefe-88c2-4d98-a0f7-eca515097827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xtr_scale, ytr, epochs=100, validation_split = 0.2, callbacks=[TrainToAccuracy(threshold=0.98, patience=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad95579-468f-400d-9007-ade962dcfcb4",
   "metadata": {},
   "source": [
    "Your model shouldn’t *really* train for 100 epochs - it should stop\n",
    "training as soon as 98% validation accuracy is achieved for 5 epochs in\n",
    "a row! (Your “test” is not graded, you may change the `threshold` and\n",
    "`patience` values in this “test” call to `model.fit` in order to check\n",
    "your work.)\n",
    "\n",
    "Note that since we are now using the validation set performance to\n",
    "*decide* when to stop training the model, we are no longer “allowed” to\n",
    "pass the test set as `validation_data`. The test set must never be used\n",
    "to make decisions during the model training process - only for\n",
    "evaluation of the final model. Instead, we specify that 20% of the\n",
    "training data should be held out as a validation set, and that is the\n",
    "validation accuracy that is used to determine when to stop training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88500e-625a-4427-9443-459747a7559f",
   "metadata": {},
   "source": [
    "### See how TTA/ETA varies with learning rate, batch size\n",
    "\n",
    "Now, you will repeat your model preparation and fitting code - with your\n",
    "new `TrainToAccuracy` callback - but in a loop. First, you will iterate\n",
    "over different learning rates.\n",
    "\n",
    "In each iteration of each loop, you will prepare a model (with the\n",
    "appropriate training hyperparameters) and train it until:\n",
    "\n",
    "-   either it has achieved **0.98 accuracy for 3 epoches in a row** on a\n",
    "    20% validation subset of the training data,\n",
    "-   or, it has trained for 500 epochs\n",
    "\n",
    "whichever comes FIRST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a7d95-a34c-4ae5-b519-9f0109f6e466",
   "metadata": {},
   "source": [
    "For each model, you will record:\n",
    "\n",
    "-   the training hyperparameters (learning rate, batch size)\n",
    "-   the number of epochs of training needed to achieve the target\n",
    "    validation accuracy\n",
    "-   the accuracy on the *test* data (not the validation data!). After\n",
    "    fitting the model, use `model.evaluate` and pass the scaled *test*\n",
    "    data to get the test loss and test accuracy\n",
    "-   **GPU runtime**: the GPU energy and time to train the model to the\n",
    "    desired validation accuracy, as computed by a `zeus-ml` measurement\n",
    "    window that starts just before `model.fit` and ends just after\n",
    "    `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de90a22-c319-444c-87e1-9c2ef07528fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO - iterate over learning rates and get TTA/ETA\n",
    "\n",
    "# default learning rate and batch size -\n",
    "batch_size = 128\n",
    "\n",
    "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "metrics_vs_lr = []\n",
    "for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    # TODO - set up model, including appropriate optimizer hyperparameters\n",
    "    \n",
    "    model_test = Sequential()\n",
    "    model_test.add(Input((Xtr_scale.shape[1],)))\n",
    "    model_test.add(Dense(nh, activation = 'sigmoid'))\n",
    "    model_test.add(Dense(len(np.unique(ytr)), activation = 'softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model_test.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "    # start measurement\n",
    "    try:\n",
    "        monitor.begin_window(\"model_train\")\n",
    "    # if the last measurement window is still running\n",
    "    except ValueError:\n",
    "        _ = monitor.end_window(\"model_train\")\n",
    "        monitor.begin_window(\"model_train\")\n",
    "\n",
    "\n",
    "    # TODO - fit model on (scaled) training data\n",
    "    # until specified validation accuracy is achieved (don't use test data!)\n",
    "    # but stop after 500 epochs even if validation accuracy is not achieved\n",
    "\n",
    "    hist_test = model_test.fit(Xtr_scale,ytr, batch_size = batch_size, epochs=500,\n",
    "                               validation_split = 0.2, callbacks=[TrainToAccuracy(threshold=0.98, patience=3)])\n",
    "\n",
    "    # end measurement\n",
    "    measurement = monitor.end_window(\"model_train\")\n",
    "\n",
    "    # TODO - evaluate model on (scaled) test data\n",
    "\n",
    "    test_acc = acc_metric(yts,model_test.predict(Xts_scale))\n",
    "\n",
    "    # save results in a dictionary\n",
    "    model_metrics = {\n",
    "       'batch_size': batch_size,\n",
    "       'learning_rate': lr,\n",
    "       'epochs': len(hist_test.history['loss']),\n",
    "       'test_accuracy': test_acc,\n",
    "       'total_energy': measurement.total_energy, # if on GPU runtime\n",
    "       'train_time': measurement.time\n",
    "    }\n",
    "\n",
    "    # TODO - append model_metrics dictionary to the metrics_vs_lr list\n",
    "    metrics_vs_lr.append(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62691663-565e-4896-8440-a5c61d8e79a9",
   "metadata": {},
   "source": [
    "Next, you will visualize the results.\n",
    "\n",
    "Create a figure with four subplots. In each subplot, create a bar plot\n",
    "with learning rate on the horizontal axis and (1) Time to accuracy, (2)\n",
    "Energy to accuracy, (3) Test accuracy, (4) Epochs, on the vertical axis\n",
    "on each subplot, respectively. Use an appropriate vertical range for\n",
    "each subplot. Label all axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67785ee9-6408-4c41-a004-0f3a06933470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - visualize effect of varying learning rate, when training to a target accuracy\n",
    "plt.figure(figsize = (9,5))\n",
    "\n",
    "lr = [_['learning_rate'] for _ in metrics_vs_lr]\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "time_ = [_['train_time'] for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=time_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Train time')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "energy_ = [_['total_energy'] for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=energy_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Total Energy')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "test_acc_ = [_['test_accuracy'].cpu().numpy() for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=test_acc_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "nepochs_ = [_['epochs'] for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=nepochs_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Num Epochs')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2877b-975e-4dae-a7b1-036ca136a33f",
   "metadata": {},
   "source": [
    "**Comment on the results**: Given that the model is trained to a target\n",
    "validation accuracy, what is the effect of the learning rate on the\n",
    "training process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0491d2-58dd-41ae-9e75-2b063fe5862d",
   "metadata": {},
   "source": [
    "**Comment** Effect of LR:\n",
    "\n",
    "-   Training time: If the LR is too high, the model does not converge,\n",
    "    and the training process stops after hitting the maximum number of\n",
    "    iterations. Hence the training time is the highest for these cases.\n",
    "    When LR is very low, the convergence occurs very slowly. So the time\n",
    "    taken is relatively higher than the case when the LR is ideal.\n",
    "-   Energy: Same trend as time. More the time, the higher the energy .\n",
    "-   Test accuracy: If the model converges, the test accuracy reaches the\n",
    "    maximum (for LR in {.0001,.001,.01}. For very high LRs, the model\n",
    "    does not converge hence the test accuracy is lower.\n",
    "-   Num epochs: Same trend as time since the batch size is the same.\n",
    "\n",
    "Now, you will repeat, with a loop over different batch sizes -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc942a5-0362-4ea6-9826-dd8156453abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - iterate over batch size and get TTA/ETA\n",
    "\n",
    "# default learning rate and batch size -\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "metrics_vs_bs = []\n",
    "for batch_size in [64, 128, 256, 512, 1024, 2048, 4096, 8192]:\n",
    "\n",
    "    # TODO - set up model, including appropriate optimizer hyperparameters\n",
    "    \n",
    "    model_test = Sequential()\n",
    "    model_test.add(Input((Xtr_scale.shape[1],)))\n",
    "    model_test.add(Dense(nh, activation = 'sigmoid'))\n",
    "    model_test.add(Dense(len(np.unique(ytr)), activation = 'softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model_test.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "    # start measurement\n",
    "    try:\n",
    "        monitor.begin_window(\"model_train\")\n",
    "    # if the last measurement window is still running\n",
    "    except ValueError:\n",
    "        _ = monitor.end_window(\"model_train\")\n",
    "        monitor.begin_window(\"model_train\")\n",
    "\n",
    "\n",
    "    # TODO - fit model on (scaled) training data\n",
    "    # until specified validation accuracy is achieved (don't use test data!)\n",
    "    # but stop after 500 epochs even if validation accuracy is not achieved\n",
    "\n",
    "    hist_test = model_test.fit(Xtr_scale,ytr, batch_size = batch_size, epochs=500,\n",
    "                               validation_split = 0.2, callbacks=[TrainToAccuracy(threshold=0.98, patience=3)])\n",
    "\n",
    "    # end measurement\n",
    "    measurement = monitor.end_window(\"model_train\")\n",
    "\n",
    "    # TODO - evaluate model on (scaled) test data\n",
    "\n",
    "    test_acc = acc_metric(yts,model_test.predict(Xts_scale))\n",
    "\n",
    "    # save results in a dictionary\n",
    "    model_metrics = {\n",
    "       'batch_size': batch_size,\n",
    "       'learning_rate': lr,\n",
    "       'epochs': len(hist_test.history['loss']),\n",
    "       'test_accuracy': test_acc,\n",
    "       'total_energy': measurement.total_energy, # if on GPU runtime\n",
    "       'train_time': measurement.time\n",
    "    }\n",
    "\n",
    "    # TODO - append model_metrics dictionary to the metrics_vs_lr list\n",
    "    metrics_vs_bs.append(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eeb46a-9d94-47ce-9d73-ae6d51d34270",
   "metadata": {},
   "source": [
    "Next, you will visualize the results.\n",
    "\n",
    "Create a figure with four subplots. In each subplot, create a bar plot\n",
    "with batch size on the horizontal axis and (1) Time to accuracy, (2)\n",
    "Energy to accuracy, (3) Test accuracy, (4) Epochs, on the vertical axis\n",
    "on each subplot, respectively. Use an appropriate vertical range for\n",
    "each subplot. Label all axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d0cad-e089-4e60-9905-71235f0bfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - visualize effect of varying batch size, when training to a target accuracy\n",
    "plt.figure(figsize = (9,5))\n",
    "batch_ls = [_['batch_size'] for _ in metrics_vs_bs]\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "time_ = [_['train_time'] for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=time_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Train time')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "energy_ = [_['total_energy'] for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=energy_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Total Energy')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "test_acc_ = [_['test_accuracy'].cpu().numpy() for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=test_acc_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "nepochs_ = [_['epochs'] for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=nepochs_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Num Epochs')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033cc1b-3b52-4a7f-85f2-3359c9d513ad",
   "metadata": {},
   "source": [
    "**Comment on the results**: Given that the model is trained to a target\n",
    "validation accuracy, what is the effect of the batch size on the\n",
    "training process in this example? What do you observe about how time and\n",
    "energy per epoch and number of epochs required varies with batch size?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
