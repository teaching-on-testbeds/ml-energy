{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for music classification\n",
    "\n",
    "**Note**: This experiment is designed to run on a Chameleon **GPU** runtime. You should use a GPU runtime launched on the Chameleon server to work on this assignment. Refer to the `reserve.ipynb` notebook for instructions on how to reserve resources and launch an instance on the Chameleon server.\n",
    "\n",
    "In this assignment, we will look at an audio classification problem. Given a sample of music, we want to determine which instrument (e.g.¬†trumpet, violin, piano) is playing.\n",
    "\n",
    "*This assignment is closely based on one by Sundeep Rangan, from his [IntroML GitHub repo](https://github.com/sdrangan/introml/).*"
   ],
   "id": "ecc91b4d-3455-4546-82d0-eead544f29a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "644871b0-44b2-4c84-a397-66b2fe420832"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Feature Extraction with Librosa\n",
    "\n",
    "The key to audio classification is to extract the correct features. The `librosa` package in python has a rich set of methods for extracting the features of audio samples commonly used in machine learning tasks, such as speech recognition and sound classification."
   ],
   "id": "6de380e9-839f-49c8-a7ea-360f1409762f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature"
   ],
   "id": "1bbebf90-4b2d-4298-a622-c4b72087f092"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use a set of music samples from the website:\n",
    "\n",
    "http://theremin.music.uiowa.edu\n",
    "\n",
    "This website has a great set of samples for audio processing.\n",
    "\n",
    "We will use the `wget` command to retrieve one file to our Google Colab storage area. (We can run `wget` and many other basic Linux commands in Colab by prefixing them with a `!` or `%`.)"
   ],
   "id": "2261fa1c-49b2-4046-bcf8-ad2dd88f01d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/SopSax.Vib.pp.C6Eb6.aiff\""
   ],
   "id": "4964b46e-3b5e-4508-b176-84ccf209d8a0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you click on the small folder icon on the far left of the Colab interface, you can see the files in your Colab storage. You should see the ‚ÄúSopSax.Vib.pp.C6Eb6.aiff‚Äù file appear there.\n",
    "\n",
    "In order to listen to this file, we‚Äôll first convert it into the `wav` format. Again, we‚Äôll use a magic command to run a basic command-line utility: `ffmpeg`, a powerful tool for working with audio and video files."
   ],
   "id": "09526a1a-e0d7-4e81-9a57-e3d456b6b129"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiff_file = 'SopSax.Vib.pp.C6Eb6.aiff'\n",
    "wav_file = 'SopSax.Vib.pp.C6Eb6.wav'\n",
    "\n",
    "!ffmpeg -y -i $aiff_file $wav_file"
   ],
   "id": "b6c6a325-a733-45bb-b86e-c5d77cf92963"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can play the file directly from the Jupyter Notebook interface. If you press the ‚ñ∂Ô∏è button, you will hear a soprano saxaphone (with vibrato) playing four notes (C, C#, D, Eb)."
   ],
   "id": "8ba22853-28c4-4c4a-baaa-28e078dd61aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(wav_file)"
   ],
   "id": "748951c6-3d07-43f8-8d58-b26f85a7d554"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `librosa` command `librosa.load` to read the audio file with filename `audio_file` and get the samples `y` and sample rate `sr`."
   ],
   "id": "3dc05451-7a74-4bba-813f-0f8b9fd47629"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(aiff_file)"
   ],
   "id": "bb4870d4-b6bd-4779-8f1a-e5e81475e8c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering from audio files is an entire subject in its own right. A commonly used set of features are called the Mel Frequency Cepstral Coefficients (MFCCs). These are derived from the so-called mel spectrogram, which is something like a regular spectrogram, but the power and frequency are represented in log scale, which more naturally aligns with human perceptual processing.\n",
    "\n",
    "You can run the code below to display the mel spectrogram from the audio sample.\n",
    "\n",
    "You can easily see the four notes played in the audio track. You also see the ‚Äòharmonics‚Äô of each notes, which are other tones at integer multiples of the fundamental frequency of each note."
   ],
   "id": "d0ce6a48-0b5b-4a56-b828-6326cbe2d2eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S),\n",
    "                         y_axis='mel', fmax=8000, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ],
   "id": "a03c652f-8c45-482a-adc1-53036dae8cc5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "Using the MFCC features described above, [Prof.¬†Juan Bello](http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello) at NYU Steinhardt and his former PhD student Eric Humphrey have created a complete data set that can used for instrument classification. Essentially, they collected a number of data files from the website above. For each audio file, the segmented the track into notes and then extracted 120 MFCCs for each note. The goal is to recognize the instrument from the 120 MFCCs. The process of feature extraction is quite involved. So, we will just use their processed data."
   ],
   "id": "4de544f8-d0ba-4283-9399-0cd504574bbc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve their data, visit\n",
    "\n",
    "<https://github.com/marl/dl4mir-tutorial/tree/master>\n",
    "\n",
    "and note the password listed on that page. Click on the link for ‚ÄúInstrument Dataset‚Äù, enter the password, click on `instrument_dataset` to open the folder, and download it. (You can ‚Äúdirect download‚Äù straight from this site, you don‚Äôt need a Dropbox account.) Depending on your laptop OS and on how you download the data, you may need to ‚Äúunzip‚Äù or otherwise extract the four `.npy` files from an archive.\n",
    "\n",
    "Now create a new folder (named `instrument_dataset`) on the Chameleon server for storing the dataset."
   ],
   "id": "ed9bdecf-b277-4897-9639-2d759d85e876"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir instrument_dataset/"
   ],
   "id": "fc260898-10f6-4e5d-a9e3-c4ecf9aca372"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, upload the files to Chamelen server inside the `instrument_dataset` folder: click on the folder icon on the left to see your storage, if it isn‚Äôt already open, and then click on ‚ÄúUpload‚Äù.\n",
    "\n",
    "üõë Wait until *all* uploads have completed and the ‚ÄúLast Modified‚Äù attribute of the file *stops updating* indicating uploads in progress are *complete*. (The training data especially will take some time to upload.) üõë"
   ],
   "id": "21aa75cf-8a2d-4d0d-bb0e-558986e6fdda"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the files with:"
   ],
   "id": "7682b81f-a42d-4038-8b36-bef9b66445e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.load('instrument_dataset/uiowa_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_train_labels.npy')\n",
    "Xts = np.load('instrument_dataset/uiowa_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ],
   "id": "6945169c-4836-470d-ac76-6f8d153fb359"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data you have just loaded in:\n",
    "\n",
    "-   How many training samples are there?\n",
    "-   How many test samples are there?\n",
    "-   What is the number of features for each sample?\n",
    "-   How many classes (i.e.¬†instruments) are there?\n",
    "\n",
    "Write some code to find these values and print them."
   ],
   "id": "7eab0aa3-9c55-49b7-9a85-08f3747f8360"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -  get basic details of the data\n",
    "# compute these values from the data, don't hard-code them\n",
    "n_tr    = Xtr.shape[0]\n",
    "n_ts    = Xts.shape[0]\n",
    "n_feat  = Xtr.shape[1]\n",
    "n_class = len(np.unique(ytr))"
   ],
   "id": "c5d3fe66-c1cc-40d6-a82b-e553635a5c81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now print those details\n",
    "print(\"Num training= %d\" % n_tr)\n",
    "print(\"Num test=     %d\" % n_ts)\n",
    "print(\"Num features= %d\" % n_feat)\n",
    "print(\"Num classes=  %d\" % n_class)"
   ],
   "id": "bdbac43f-db15-483d-9cfe-4c03fb776973"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training set\n",
    "# (when loaded in, samples are ordered by class)\n",
    "p = np.random.permutation(Xtr.shape[0])\n",
    "Xtr = Xtr[p,:]\n",
    "ytr = ytr[p]"
   ],
   "id": "16477fb0-9989-4aef-bb29-7b59f2522cb4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, standardize the training and test data, `Xtr` and `Xts`, by removing the mean of each feature and scaling to unit variance.\n",
    "\n",
    "You can do this manually, or using `sklearn`‚Äôs [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). (For an example showing how to use a `StandardScaler`, you can refer to the notebook on regularization.)\n",
    "\n",
    "Although you will scale both the training and test data, you should make sure that both are scaled according to the mean and variance statistics from the *training data only*.\n",
    "\n",
    "<small>Standardizing the input data can make the gradient descent work better, by making the loss function ‚Äúeasier‚Äù to descend.</small>"
   ],
   "id": "db7327ff-6d13-4caf-9477-9fc6a5adab5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "b890c844-f457-4e63-af6a-aef068860814"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ],
   "id": "46f1c724-23d1-40c6-bfe5-900221ff3fa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Standardize the training and test data\n",
    "Xtr_scale = scaler.fit_transform(Xtr)\n",
    "Xts_scale = scaler.transform(Xts)"
   ],
   "id": "c60b3d5b-015b-4a7d-a6ce-d044415ff3af"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the standardized training and test data features for further use."
   ],
   "id": "3a42e563-5d30-4dd6-8f79-2ec48fe5f64b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('instrument_dataset/uiowa_std_scale_train_data.npy',Xtr_scale)\n",
    "np.save('instrument_dataset/uiowa_std_scale_test_data.npy',Xts_scale)\n",
    "np.save('instrument_dataset/uiowa_permuted_train_labels.npy',ytr)"
   ],
   "id": "2951e417-fe6c-47fa-987a-039d4047f033"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network Classifier\n",
    "\n",
    "Create a neural network `model` with:\n",
    "\n",
    "-   `nh=256` hidden units in a single dense hidden layer\n",
    "-   `sigmoid` activation at hidden units\n",
    "-   select the input and output shapes, and output activation, according to the problem requirements."
   ],
   "id": "944e4f80-05fa-4ced-8901-269715b4c07b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "id": "c5af4cfc-712a-40ed-b19d-2ec5bd41fc3c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Here, we‚Äôll load the processed data defined in the previous notebook"
   ],
   "id": "29b11e7c-33fc-4a89-8afb-cdad48a1513e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_scale = np.load('instrument_dataset/uiowa_std_scale_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_permuted_train_labels.npy')\n",
    "Xts_scale = np.load('instrument_dataset/uiowa_std_scale_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ],
   "id": "b6e6f203-cee3-45b5-8406-32117c243106"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classification model"
   ],
   "id": "ba0dea26-2df0-400e-9433-7b68c08a83af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow.keras.backend as K"
   ],
   "id": "906481fa-5e62-4f0d-a5af-b42fb935c762"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - construct the model\n",
    "nh = 256\n",
    "model = Sequential()\n",
    "model.add(Input(Xtr_scale.shape[1],))\n",
    "model.add(Dense(nh, activation = 'sigmoid'))\n",
    "model.add(Dense(len(np.unique(ytr)), activation = 'softmax'))"
   ],
   "id": "93dca728-87b9-450c-9751-3e439d6724e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model summary."
   ],
   "id": "d0fdb0c4-eb93-4fca-a77b-aeea503b13ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model summary\n",
    "model.summary()"
   ],
   "id": "17e8226a-7a44-4842-9e76-fa90bebd4dc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also visualize the model with\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ],
   "id": "fe33a857-92df-443c-830a-9b5ab8460ea3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer and compile the model. Select the appropriate loss function for this multi-class classification problem, and use an accuracy metric. For the optimizer, use the Adam optimizer with a learning rate of 0.001"
   ],
   "id": "ccba2e19-367b-45c2-8afe-cc03b178989a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - create optimizer and compile the model\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])"
   ],
   "id": "e08511be-50d9-48d9-8121-f5e58302378f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for 10 epochs using the scaled data for both training and validation, and save the training history in \\`hist.\n",
    "\n",
    "Use the `validation_data` option to pass the *test* data. (This is OK because we are not going to use this data as part of the training process, such as for early stopping - we‚Äôre just going to compute the accuracy on the data so that we can see how training and test loss changes as the model is trained.)\n",
    "\n",
    "Use a batch size of 128. Your final accuracy should be greater than 99%."
   ],
   "id": "75181eca-b829-4dde-94e9-daef18a8caf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fit model and save training history\n",
    "n_epochs = 10\n",
    "\n",
    "hist = model.fit(Xtr_scale,ytr, batch_size = 128, epochs = 10, validation_data=(Xts_scale, yts))"
   ],
   "id": "d1c13ff2-3998-40db-82f9-dc1a71359e81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy saved in `hist.history` dictionary, on the same plot. This gives one accuracy value per epoch. You should see that the validation accuracy saturates around 99%. After that it may ‚Äúbounce around‚Äù a little due to the noise in the stochastic mini-batch gradient descent.\n",
    "\n",
    "Make sure to label each axis, and each series (training vs.¬†validation/test)."
   ],
   "id": "e4261e45-5f20-4fc1-ad04-946af7b9a0be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - plot the training and validation accuracy in one plot\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "train_acc = hist.history['accuracy'];\n",
    "val_acc = hist.history['val_accuracy'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "plt1 = sns.lineplot(x=np.arange(1,nepochs+1), y=train_acc, label='Training accuracy');\n",
    "plt2 = sns.lineplot(x=np.arange(1,nepochs+1), y=val_acc, label='Validation accuracy');\n",
    "xlab = plt.xlabel('Epoch');\n",
    "ylab = plt.ylabel('Accuracy')"
   ],
   "id": "1c0c1760-4fda-45d5-a60c-184e50a6a511"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss values saved in the `hist.history` dictionary, on the same plot. You should see that the training loss is steadily decreasing. Use the [`semilogy` plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogy.html) so that the y-axis is log scale.\n",
    "\n",
    "Make sure to label each axis, and each series (training vs.¬†validation/test)."
   ],
   "id": "95b41b64-af30-4835-b2dc-92a3d06f0a86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - plot the training and validation loss in one plot\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "train_loss = hist.history['loss'];\n",
    "val_loss = hist.history['val_loss'];\n",
    "\n",
    "nepochs = len(train_acc);\n",
    "plt1 = plt.semilogy(np.arange(1,nepochs+1), train_loss, label='Training loss');\n",
    "plt2 = plt.semilogy(np.arange(1,nepochs+1), val_loss, label='Validation loss');\n",
    "xlab = plt.xlabel('Epoch')\n",
    "ylab = plt.ylabel('Loss')\n",
    "lgnd = plt.legend()"
   ],
   "id": "3c73af70-ec76-429e-867b-59a309c49e58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy and time to train a neural network\n",
    "\n",
    "In the previous notebook, we trained a model to classify music. Now, we‚Äôll explore what happens when we vary the training hyperparameters, but train each model to the same validation **accuracy target**. We will consider:\n",
    "\n",
    "-   how much *time* it takes to achieve that accuracy target (‚Äútime to accuracy‚Äù)\n",
    "-   how much *energy* it takes to achieve that accuracy target (‚Äúenergy to accuracy‚Äù)\n",
    "-   and the *test accuracy* for the model, given that it is trained to the specified validation accuracy target"
   ],
   "id": "ee598733-fac3-4de0-bf2c-cb70fa984486"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "id": "c9eb4c11-bfa0-4bbd-809f-56e5b08b501f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow.keras.backend as K"
   ],
   "id": "5a119835-966b-4ca2-8163-6c2fc0720302"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Here, we‚Äôll load the processed data defined in the previous notebook"
   ],
   "id": "b2c99ee3-fd34-4ec7-8262-9c4120066ba8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_scale = np.load('instrument_dataset/uiowa_std_scale_train_data.npy')\n",
    "ytr = np.load('instrument_dataset/uiowa_permuted_train_labels.npy')\n",
    "Xts_scale = np.load('instrument_dataset/uiowa_std_scale_test_data.npy')\n",
    "yts = np.load('instrument_dataset/uiowa_test_labels.npy')"
   ],
   "id": "ee5d5d1b-804b-4cdb-8905-26d6b5862b90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 256 #Number of units for the hidden layer of the neural network"
   ],
   "id": "1f7c838c-0751-4438-8538-8cfc5a767a64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy consumption\n",
    "\n",
    "To do this, first we will need some way to measure the energy used to train the model. We will use [Zeus](https://ml.energy/zeus/overview/), a Python package developed by researchers at the University of Michigan, to measure the GPU energy consumption."
   ],
   "id": "44c99ab6-8990-4f24-b884-0c54def8f76a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the zeus-ml package, and tell it to monitor your GPU:"
   ],
   "id": "86ab0af8-5cc8-46e2-b3a5-a0c1522fd717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeus.monitor import ZeusMonitor\n",
    "\n",
    "monitor = ZeusMonitor()"
   ],
   "id": "e709101f-2539-4623-86a4-ec9defd5bada"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to measure GPU energy usage, you will:\n",
    "\n",
    "-   start a ‚Äúmonitoring window‚Äù\n",
    "-   do your GPU-intensive computation (e.g.¬†call `model.fit`)\n",
    "-   stop the ‚Äúmonitoring window‚Äù\n",
    "\n",
    "and then you can get the time and total energy used by the GPU in the monitoring window.\n",
    "\n",
    "Try it now - this will just continue fitting whatever `model` is currently in scope from previous cells:"
   ],
   "id": "3824e73c-6e63-42ad-9403-4e65e74784ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((Xtr_scale.shape[1],)))\n",
    "model.add(Dense(nh, activation = 'sigmoid'))\n",
    "model.add(Dense(len(np.unique(ytr)), activation = 'softmax'))\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "try:\n",
    "    monitor.begin_window(\"test\")\n",
    "# if the last measurement window is still running\n",
    "except ValueError:\n",
    "    _ = monitor.end_window(\"test\")\n",
    "    monitor.begin_window(\"test\")\n",
    "\n",
    "model.fit(Xtr_scale, ytr, epochs=5)\n",
    "measurement = monitor.end_window(\"test\")\n",
    "print(\"Measured time (s)  :\" , measurement.time)\n",
    "print(\"Measured energy (J):\" , measurement.total_energy)"
   ],
   "id": "10db7ae3-86a9-4eb7-9a84-79cb031c7499"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TrainToAccuracy` callback\n",
    "\n",
    "Next, we need a way to train a model until we achieve our desired validation accuracy. We will [write a callback function](https://www.tensorflow.org/guide/keras/writing_your_own_callbacks) following these specifications:\n",
    "\n",
    "-   It will be called `TrainToAccuracy` and will accept two arguments: a `threshold` and a `patience` value.\n",
    "-   If the model‚Äôs validation accuracy is higher than the `threshold` for `patience` epochs in a row, stop training.\n",
    "-   In the `on_epoch_end` function, which will be called at the end of every epoch during training, you should get the current validation accuracy using `currect_acc = logs.get(\"val_accuracy\")`. Then, set `self.model.stop_training = True` if the condition above is met.\n",
    "-   The default values of `threshold` and `patience` are given below, but other values may be passed as arguments at runtime.\n",
    "\n",
    "Then, when you call `model.fit()`, you will add the `TrainToAccuracy` callback as in\n",
    "\n",
    "    callbacks=[TrainToAccuracy(threshold=0.98, patience=5)]"
   ],
   "id": "4945d423-bf52-4cad-a2af-459ae41d5a80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - write a callback function\n",
    "class TrainToAccuracy(callbacks.Callback):\n",
    "\n",
    "    def __init__(self, threshold=0.9, patience=3):\n",
    "        self.threshold = threshold  # the desired accuracy threshold\n",
    "        self.patience = patience # how many epochs to wait once hitting the threshold\n",
    "        self.last_change_epoch = -1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_acc = logs.get(\"val_accuracy\")\n",
    "        if(current_acc>self.threshold):\n",
    "          if(epoch - self.last_change_epoch>=self.patience):\n",
    "            self.model.stop_training = True\n",
    "        else:\n",
    "          self.last_change_epoch = epoch\n",
    "        return"
   ],
   "id": "77b2818f-fed3-4fd1-9836-e76f676af321"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it! run the following cell to test your `TrainToAccuracy` callback. (This will just continue fitting whatever `model` is currently in scope.)"
   ],
   "id": "9eee624e-2bdc-48ae-9078-0693d60d7798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xtr_scale, ytr, epochs=100, validation_split = 0.2, callbacks=[TrainToAccuracy(threshold=0.98, patience=5)])"
   ],
   "id": "2d5e879b-3e5f-42cd-a445-480f34e0f9b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model shouldn‚Äôt *really* train for 100 epochs - it should stop training as soon as 98% validation accuracy is achieved for 5 epochs in a row! (Your ‚Äútest‚Äù is not graded, you may change the `threshold` and `patience` values in this ‚Äútest‚Äù call to `model.fit` in order to check your work.)\n",
    "\n",
    "Note that since we are now using the validation set performance to *decide* when to stop training the model, we are no longer ‚Äúallowed‚Äù to pass the test set as `validation_data`. The test set must never be used to make decisions during the model training process - only for evaluation of the final model. Instead, we specify that 20% of the training data should be held out as a validation set, and that is the validation accuracy that is used to determine when to stop training."
   ],
   "id": "c97718fd-be6f-41e6-936d-97b0026b4d55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how TTA/ETA varies with learning rate, batch size\n",
    "\n",
    "Now, you will repeat your model preparation and fitting code - with your new `TrainToAccuracy` callback - but in a loop. First, you will iterate over different learning rates.\n",
    "\n",
    "In each iteration of each loop, you will prepare a model (with the appropriate training hyperparameters) and train it until:\n",
    "\n",
    "-   either it has achieved **0.98 accuracy for 3 epoches in a row** on a 20% validation subset of the training data,\n",
    "-   or, it has trained for 500 epochs\n",
    "\n",
    "whichever comes FIRST."
   ],
   "id": "5f03beb7-38d9-43e8-92a3-b6a200d62447"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, you will record:\n",
    "\n",
    "-   the training hyperparameters (learning rate, batch size)\n",
    "-   the number of epochs of training needed to achieve the target validation accuracy\n",
    "-   the accuracy on the *test* data (not the validation data!). After fitting the model, use `model.evaluate` and pass the scaled *test* data to get the test loss and test accuracy\n",
    "-   **GPU runtime**: the GPU energy and time to train the model to the desired validation accuracy, as computed by a `zeus-ml` measurement window that starts just before `model.fit` and ends just after `model.fit`."
   ],
   "id": "c0d81da5-a403-497e-9f58-cd6eeb97060c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO - iterate over learning rates and get TTA/ETA\n",
    "\n",
    "# default learning rate and batch size -\n",
    "batch_size = 128\n",
    "\n",
    "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "metrics_vs_lr = []\n",
    "for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    # TODO - set up model, including appropriate optimizer hyperparameters\n",
    "    \n",
    "    model_test = Sequential()\n",
    "    model_test.add(Input((Xtr_scale.shape[1],)))\n",
    "    model_test.add(Dense(nh, activation = 'sigmoid'))\n",
    "    model_test.add(Dense(len(np.unique(ytr)), activation = 'softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model_test.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "    # start measurement\n",
    "    try:\n",
    "        monitor.begin_window(\"model_train\")\n",
    "    # if the last measurement window is still running\n",
    "    except ValueError:\n",
    "        _ = monitor.end_window(\"model_train\")\n",
    "        monitor.begin_window(\"model_train\")\n",
    "\n",
    "\n",
    "    # TODO - fit model on (scaled) training data\n",
    "    # until specified validation accuracy is achieved (don't use test data!)\n",
    "    # but stop after 500 epochs even if validation accuracy is not achieved\n",
    "\n",
    "    hist_test = model_test.fit(Xtr_scale,ytr, batch_size = batch_size, epochs=500,\n",
    "                               validation_split = 0.2, callbacks=[TrainToAccuracy(threshold=0.98, patience=3)])\n",
    "\n",
    "    # end measurement\n",
    "    measurement = monitor.end_window(\"model_train\")\n",
    "\n",
    "    # TODO - evaluate model on (scaled) test data\n",
    "\n",
    "    test_acc = acc_metric(yts,model_test.predict(Xts_scale))\n",
    "\n",
    "    # save results in a dictionary\n",
    "    model_metrics = {\n",
    "       'batch_size': batch_size,\n",
    "       'learning_rate': lr,\n",
    "       'epochs': len(hist_test.history['loss']),\n",
    "       'test_accuracy': test_acc,\n",
    "       'total_energy': measurement.total_energy, # if on GPU runtime\n",
    "       'train_time': measurement.time\n",
    "    }\n",
    "\n",
    "    # TODO - append model_metrics dictionary to the metrics_vs_lr list\n",
    "    metrics_vs_lr.append(model_metrics)"
   ],
   "id": "9e4e23ba-a4ba-4efb-b3b0-a6475e8a46ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will visualize the results.\n",
    "\n",
    "Create a figure with four subplots. In each subplot, create a bar plot with learning rate on the horizontal axis and (1) Time to accuracy, (2) Energy to accuracy, (3) Test accuracy, (4) Epochs, on the vertical axis on each subplot, respectively. Use an appropriate vertical range for each subplot. Label all axes."
   ],
   "id": "84b3898e-1593-45d3-8420-ec50e7a23a16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - visualize effect of varying learning rate, when training to a target accuracy\n",
    "plt.figure(figsize = (9,5))\n",
    "\n",
    "lr = [_['learning_rate'] for _ in metrics_vs_lr]\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "time_ = [_['train_time'] for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=time_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Train time')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "energy_ = [_['total_energy'] for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=energy_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Total Energy')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "test_acc_ = [_['test_accuracy'].cpu().numpy() for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=test_acc_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "nepochs_ = [_['epochs'] for _ in metrics_vs_lr]\n",
    "\n",
    "sns.barplot(x=lr, y=nepochs_)\n",
    "plt.xlabel('LR');\n",
    "plt.ylabel('Num Epochs')\n",
    "plt.tight_layout()"
   ],
   "id": "615bfeeb-9dbb-4a7c-9a2f-05dc64e2ba10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the results**: Given that the model is trained to a target validation accuracy, what is the effect of the learning rate on the training process?"
   ],
   "id": "ae9ceaae-2d53-4c1f-97f9-11a557779c35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment** Effect of LR:\n",
    "\n",
    "-   Training time: If the LR is too high, the model does not converge, and the training process stops after hitting the maximum number of iterations. Hence the training time is the highest for these cases. When LR is very low, the convergence occurs very slowly. So the time taken is relatively higher than the case when the LR is ideal.\n",
    "-   Energy: Same trend as time. More the time, the higher the energy .\n",
    "-   Test accuracy: If the model converges, the test accuracy reaches the maximum (for LR in {.0001,.001,.01}. For very high LRs, the model does not converge hence the test accuracy is lower.\n",
    "-   Num epochs: Same trend as time since the batch size is the same.\n",
    "\n",
    "Now, you will repeat, with a loop over different batch sizes -"
   ],
   "id": "173acc03-d12c-41c7-bf25-0585921a49fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - iterate over batch size and get TTA/ETA\n",
    "\n",
    "# default learning rate and batch size -\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "metrics_vs_bs = []\n",
    "for batch_size in [64, 128, 256, 512, 1024, 2048, 4096, 8192]:\n",
    "\n",
    "    # TODO - set up model, including appropriate optimizer hyperparameters\n",
    "    \n",
    "    model_test = Sequential()\n",
    "    model_test.add(Input((Xtr_scale.shape[1],)))\n",
    "    model_test.add(Dense(nh, activation = 'sigmoid'))\n",
    "    model_test.add(Dense(len(np.unique(ytr)), activation = 'softmax'))\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model_test.compile(optimizer = opt, loss = loss_fn, metrics = ['accuracy'])\n",
    "\n",
    "    # start measurement\n",
    "    try:\n",
    "        monitor.begin_window(\"model_train\")\n",
    "    # if the last measurement window is still running\n",
    "    except ValueError:\n",
    "        _ = monitor.end_window(\"model_train\")\n",
    "        monitor.begin_window(\"model_train\")\n",
    "\n",
    "\n",
    "    # TODO - fit model on (scaled) training data\n",
    "    # until specified validation accuracy is achieved (don't use test data!)\n",
    "    # but stop after 500 epochs even if validation accuracy is not achieved\n",
    "\n",
    "    hist_test = model_test.fit(Xtr_scale,ytr, batch_size = batch_size, epochs=500,\n",
    "                               validation_split = 0.2, callbacks=[TrainToAccuracy(threshold=0.98, patience=3)])\n",
    "\n",
    "    # end measurement\n",
    "    measurement = monitor.end_window(\"model_train\")\n",
    "\n",
    "    # TODO - evaluate model on (scaled) test data\n",
    "\n",
    "    test_acc = acc_metric(yts,model_test.predict(Xts_scale))\n",
    "\n",
    "    # save results in a dictionary\n",
    "    model_metrics = {\n",
    "       'batch_size': batch_size,\n",
    "       'learning_rate': lr,\n",
    "       'epochs': len(hist_test.history['loss']),\n",
    "       'test_accuracy': test_acc,\n",
    "       'total_energy': measurement.total_energy, # if on GPU runtime\n",
    "       'train_time': measurement.time\n",
    "    }\n",
    "\n",
    "    # TODO - append model_metrics dictionary to the metrics_vs_lr list\n",
    "    metrics_vs_bs.append(model_metrics)"
   ],
   "id": "4d2a9809-0e3f-4bd6-9a1d-c3f8fc47361a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will visualize the results.\n",
    "\n",
    "Create a figure with four subplots. In each subplot, create a bar plot with batch size on the horizontal axis and (1) Time to accuracy, (2) Energy to accuracy, (3) Test accuracy, (4) Epochs, on the vertical axis on each subplot, respectively. Use an appropriate vertical range for each subplot. Label all axes."
   ],
   "id": "23fee6ad-01a0-4de4-a7df-2fdf46602c61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - visualize effect of varying batch size, when training to a target accuracy\n",
    "plt.figure(figsize = (9,5))\n",
    "batch_ls = [_['batch_size'] for _ in metrics_vs_bs]\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "time_ = [_['train_time'] for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=time_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Train time')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "energy_ = [_['total_energy'] for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=energy_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Total Energy')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "test_acc_ = [_['test_accuracy'].cpu().numpy() for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=test_acc_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "nepochs_ = [_['epochs'] for _ in metrics_vs_bs]\n",
    "\n",
    "sns.barplot(x=batch_ls, y=nepochs_)\n",
    "plt.xlabel('Batch Size');\n",
    "plt.ylabel('Num Epochs')\n",
    "plt.tight_layout()"
   ],
   "id": "5a21efc0-a581-4477-9bc2-48b84f187eb3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the results**: Given that the model is trained to a target validation accuracy, what is the effect of the batch size on the training process in this example? What do you observe about how time and energy per epoch and number of epochs required varies with batch size?"
   ],
   "id": "6a8814dd-03ca-4eff-878a-70a2b8d18295"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
